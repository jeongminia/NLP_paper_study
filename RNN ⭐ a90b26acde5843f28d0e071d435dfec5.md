# RNN ⭐

---

<aside>
💡 ***RNN*  순환신경망
R**ecurrent **n**eural **n**etwork based language model

</aside>

             : 이해되지 않는 부분

             : 중요 부분

---

## 밑바닥부터 시작하는 딥러닝2

![스크린샷 2024-04-05 오후 4.49.47.png](RNN%20%E2%AD%90%20a90b26acde5843f28d0e071d435dfec5/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2024-04-05_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_4.49.47.png)

![fig 5-8.png](RNN%20%E2%AD%90%20a90b26acde5843f28d0e071d435dfec5/fig_5-8.png)

- 시간 개념을 반영할 수 있는 인공 신경망
- 은닉 정보를 업데이트해가며 연속된 입력 처리
- 데이터가 순환하면서 정보가 끊임없이 갱신
- 순환 경로를 따라 데이터는 끊임없이 순환하며 과거의 정보를 기억하는 동시에 최신 데이터로 갱신
    - $X_t$
        - $(x_0, x_1, …, x_t, …)$ 입력값이며 시계열 데이터에 해당
            - $x_t$
                
                각 단어의 분산 표현
                
        - 각각의 벡터가 모여 행렬
    - $h_t$
        - $(h_0, h_1, …, h_t, …)$ 입력에 대응되는 출력값
- 시계열 데이터는 시간 방향으로 데이터 나열
- t 시점의 RNN 계층은 그 계층의 입력 $x_t$(행벡터)과 1개의 전의 RNN계층으로부터의 출력
    
    $h_{t-1}$ (행벡터)을 받음
    
- $h_t = tanh(h_{t-1}W_h + x_tW_x+b)$

## 0. Abstract

---

- 음성 인식에 적용할 수 있는 새로운 순환 신경망 기반 언어모델
- 여러 RNN LMs RNN 기반 언어 모델을 함께 사용하면 최신 Backoff LM에 비해 perplexity를 약 50% 감소할 수 있어 RNN기반 언어모델 성능이 더 좋음
    - [Backoff LM](https://sotudy.tistory.com/28)
        - **정의** :  n-gram 기반의 통계적 언어 모델
            - n-gram :  n번째 이전의 단어가 주어졌을 때 n번째 단어가 나올 확률로 언어모델
        - **등장 배경** : 특정 n-gram이 훈련 데이터에서 충분히 나타나지 않아, 데이터 희소성 문제를 해결하기 위함
        - **특징** : 더 짧은 n-gram의 확률로 back-off해 확률 추정
            - back-off :  n-gram의 확률이 0이면 n-1 gram을 사용하는 방식
        - **단점** : 계산이 간단하고 효율적이나 복잡한 언어 패턴을 학습하는 데 한계
    - [perplexity](https://wikidocs.net/21697)
        - 정의 : 언어 모델의 성능을 측정하는 지표로 값이 낮을수록 모델을 더 정확히 예측
        - 좋은 언어 모델이란?
            
            ⇒ 잘 정의된 테스트셋의 문장에 대해서 높은 확률을 반환하는 언어모델!
            
- 훈련에 높은 계산 복잡도

## 1. Introduction

---

- 연속적인 데이터 예측은 ML, DL의 핵심 문제 [1]
- 통계적 언어 모델링 LM
    - 목표 : 주어진 문맥 내, 텍스트 데이터의 다음 단어를 예측하는 것
        
        → 언어 모델을 구성할 때 sequential data prediction 문제를 다루고 있음
        
    - 매우 특화된 접근 방식을 포함 - 몇가지 기본적인 가정들을 보유
        - ex. n-gram
            - 언어가 단어라는 원자 기호의 연속으로 구성
            - 문장의 끝 기호가 매우 특별한 역할을 함
- **cache models** 과 **class-based models** 의 도입으로 상당한 개선이 이뤄짐
    - **cache models**
        - 긴 문맥 정보를 설명하며 이전에 나온 단어들을 기억해 두었다가 그 단어들이 나올 확률을 높이는 방식 → 단어들의 빈도와 같은 정보를 활용해 다음 단어 예측
        - 긴 문맥에서 일관성을 유지하는 데에 도움이 됨
    - **class-based models**
        - 유사 단어들 간에 매개변수를 공유해 짧은 문맥에서의 매개변수 추정을 개선
        - ex.
            
            고양이, 개, 토끼 를 “동물”로 묶어 정보를 공유해 단어들의 예측 정확도를 높임
            

⏩ 한계점

- 고급 언어 모델링 기술이 실제 응용에서 성공을 거두는 데 어려움을 겪고 있음
    
    → 실용성이 낮음
    
- 이유 : 실제 응용에서는 너무나도 많은 데이터로 구축되는 데, 데이터가 많을수록 더 좋은 모델이 만들어지는 것은 아님

⇒ 실제 응용에서는 데이터가 많을 때 간단한 모델들보다도 더 큰 차이를 만들지 못하는 경우가 많음

## 2. Model description

---

- 순차 데이터(텍스트, 음성, 주가 등)을 효과적으로 모델링하기 위해 RNN을 사용
- RNN은 이전의 입력을 기억하고 다음 입력에 영향을 줄 수 있는 구조를 갖고 있어 순차 데이터에 적합
- Bengio의 연구
    - 인공신경망을 사용한 통계적 언어 모델링 제안
    - **고정 길이의 context**를 사용하는 피드포워드 신경망 사용
        - 고정 길이 context : 문장 내에 일정한 길이의 단어들을 한번에 처리
    - 한계
        - 피드포워드 신경망이 고정길이 context를 사용해야 한다는 점이 주요 단점
        - 해당 길이는 훈련 전에 임의로 지정되어야 하며 일반적으로 신경망이 다음 단어를 예측할 때 앞 5-10개의 단어만 고려하게 됨
        - but. 인간은 더 긴 context를 효과적으로 활용할 수 있어 인간의 언어 처리 능력과 비교할 때 중요 단점으로 작용
- Schwenk, Goodman의 연구
    - 음성 인식(소리 신호를 텍스트로 변환하는 작업)에서도 효과적으로 신경망 기반 모델이 좋은 성능을 보임
- 캐시 모델과 신경망 모델
    - 캐시 모델은 최근에 관찰된 단어들의 빈도와 같은 정보를 활용해 다음 단어 예측
    - 캐시 모델과 신경망 모델을 결합하면 보완할 수 있는 정보를 제공받아 좋은 성능을 낼 수 있어 임의 길이의 맥락을 처리할 수 있는 모델의 필요 → RNN

⏩ 순환신경망은 순차 데이터를 사용하는데 매우 적합한 모델이며 과거 연구들은 인공 신경망을 사용한 언어모델링과 음성인식에서 뛰어난 성능을 보임

### RNN

- 제한된 크기의 context를 사용하지 않음
- 순환 연결을 이용해 정보는 해당 네트워크 내에서 임의로 오랜 시간동안 순환할 수 있음
- but. 확률적 경사하강법을 통해 장기 의존성을 학습하는 것은 매우 어려울 수 있다는 주장이 자주 제기됨
    
    ⇒ LSTM 등장
    
- 구조
    - Elman network 구조 or 가장 단순한 버전의 순환 신경망 SRNN 을 사용
        
        → RNN의 가장 기본적인 형태로 구현 및 훈련이 매우 쉬움
        
    - $x$ 입력층 / $s$ 은닉층 / $y$ 출력층
    - $x(t)$는 시간 t에서의 네트워크 입 벡터로 현재 단어를 나타내는 벡터 w와 시간 t-1에서 context layer의 뉴런 출력의 연결로 형성
- 수식
    - **입력벡터**  $x(t)=w(t)+s(t−1)$
        - 현재 단어 벡터 $w(t)$와 이전 은닉층 상태 벡터 $s(t-1)$을 더해 현재 시간 t에서의 입력벡터 $x(t)$ 형성
        - 고유한 위치는 , 나머지는 0을 두는 방식으로 인코딩
        - 벡터의 크기는, vocab_size와 context layer를 더한 값
        - 은닉층의 크기는 훈련 데이터의 양에 비례해야 함
    - **은닉층** 상태 계산
        - 입력 벡터의 각 요소와 가중치의 곱을 모두 더한 후 sigmoid 함수를 적용해 은닉층 상태 계산
            - 시그모이드 활성화 함수 : 출력값 0 ~ 1
        - 초기 은닉 상태 s(0) : 데이터 양이 많을 때 초기화는 크게 중요하지 않지만 작은 값들로 일반적으로 설정함
        - s(t+1) : 다음 단계 t+1에서의 은닉 상태는 이전 은닉 상태의 copy
        - 매우 큰 은닉층을 사용해도 네트워크가 크게 과적합되지 않아 네트워크 규제는 큰 개선을 제공하지 않음
        
        ⇒ 훈련전에 조정해야 하는 파라미터가 오직 은닉층(context layer)의 크기
        
    - **출력층** 상태 계산
        - 은닉층의 각 요소와 가중치의 곱을 모두 더한 후 softmax 활성화 함수를 적용해 출력층 상태 계산
            - 소프트맥스 활성화함수 : 출력값 0 ~ 1로 모든 출력값의 합이 1이 되도록 해 다중 클래스 분류에서 사용
- network 훈련
    - epoch : 훈련 코퍼스의 모든 데이터를 순차적으로 제공하며 여러 에폭동안 훈련
        - 유의미한 개선이 없으면 학습률은 새로운 epoch 시작 시 학습률이 반으로 감소하며 훈련 종료
        - 보통 10-20 에폭 후 수렴 이뤄짐
    - 가중치 : 가장 작은 값으로 초기화
    - 훈련 알고리즘 : 확률적 경사하강법을 사용하는 방식으로 훈련
- 오류 벡터기반 가중치 업데이트
    - 오류벡터 : 각 훈련 단계에서 cross entropy 기준으로 오류 벡터를 계산하고 표준 역전파 알고리즘을 사용해 가중치 업데이트
- 다이나믹 모델
    - 고정된 학습률 0.01을 사용하며 훈련
        - ex. dog cat → dog 확률이 증가하면 cat의 확률도 증가
    - 새로운 도메인에 자동적으로 적용할 수 있음 ⭐
    - dynamic RNN (deprecated) vs static RNN
        - 문장의 길이(seq_len)만큼 recurrent가 일어나지만 정적은 고정
        - 별도로 각 문장마다 길이가 얼마인지를 명시해줘야 하지만 정적은 별도의 패딩 필요

단순 순환 신경망(Elman Network)은 순차 데이터를 효과적으로 학습할 수 있습니다. 초기화, 입력 벡터의 구성, 은닉층의 크기 설정, 그리고 훈련 절차 모두가 네트워크의 성능에 중요한 영향을 미칩니다.

### 2.1 Optimization

- 훈련 데이터에서 자주 등장하지 않는 단어들을 단순화하고 처리 효율성을 높이는 방법으로는 임곗값 이하로 자주 등장하는 모든 단어들을 하나의 특수한 토큰으로 병합

![[표1] 훈련 데이터 크기가 증가할 때 WSJ DEV 세트에서 모델의 성능
다양한 언어 모델의 성능과 훈련 데이터의 양이 증가함에 따라 성능 지표가 어떻게 변하는지 보여줌](RNN%20%E2%AD%90%20a90b26acde5843f28d0e071d435dfec5/Untitled.png)

[표1] 훈련 데이터 크기가 증가할 때 WSJ DEV 세트에서 모델의 성능
다양한 언어 모델의 성능과 훈련 데이터의 양이 증가함에 따라 성능 지표가 어떻게 변하는지 보여줌

- 설명
    - **KN5 LM :**  Kneser-Ney 5-gram 언어 모델
    - **RNN :** 순환 신경망을 나타내며, 숫자는 레이어와 유닛 수

## 3. WSJ experiments

---

Wall Street Journal (WSJ) 데이터 세트

![스크린샷 2024-07-14 오전 8.52.39.png](RNN%20%E2%AD%90%20a90b26acde5843f28d0e071d435dfec5/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2024-07-14_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%258C%25E1%2585%25A5%25E1%2586%25AB_8.52.39.png)

1. 훈련 데이터 및 실험 설정
    - **데이터 출처** : English Gigaword의 NYT 섹션에서총 3,700만 단어
    - **훈련 데이터 크기**: RNN은 큰 데이터로 훈련하는 것은 시간이 많이 소요되므로, 최대 640만 단어(=약 30만 문장)만 사용
    - **퍼플렉서티 평가**: 23만 단어의 테스트 데이터에서 퍼플렉서티를 평가
2. 모델 결합 및 보간
    - **모델 결합 방법** : RNN LM에 0.75, backoff LM에 0.25의 가중치를 주어 선형 보간
        
        → 퍼플렉서티(PPL)와 단어 오류율(WER)을 평가
        
    - **결합 모델** : 다양한 아키텍처의 여러 RNN LM의 출력을 선형 보간
        - **복잡한 모델일수록 성능 향상**: RNN의 은닉층 크기와 희귀 토큰 임계값에 따라 성능이 향상
        - 모델 결합 효과 : 단일 RNN모델보다 더 나은 성능을 보임
3. 성능 결과
    - **Oracle WER**: dev set 6.1%, eval set 9.5%로 보고되었으며 약 18퍼 감소
        - RNN+KN 모델이 RNN 모델보다 더 낮은 WER을 보임.
    - **Perplexity**: 테스트 데이터(23만 단어)에 대해 평가했으며 약 50퍼 감소
        - 일반적으로 RNN+KN 모델이 RNN 모델보다 퍼플렉서티가 낮아 더 나은 성능을 보임

## 4. NIST RT05 experiments

---

1. 문제 상황  
    - 앞선 실험에서 사용된 모델이 최신 상태가 아니였으며, 사용된 데이터가 충분하지 않음
2. 훈련 데이터 및 실험 설정
    - **데이터 출처** : NIST RT05 평가에 사용된 AMI 시스템의 라티스를 통해 115시간의 음향
3. 실험 설정
    - 교차 단어 결합 상태를 기반으로 한 특정 유형의 음향 모델과 다양한 데이터로 훈련된 언어모델 사용
    - 최신 시스템에서도 의미 있는 개선을 입증하기 위해 NIST RT05 평가를 통해 성능 비교
    - **RT05 LM**:
        - **총 훈련 데이터 양**: 1.3G 단어 이상
        - **데이터 출처**: 다양한 데이터 소스 (자세한 내용은 [13] 참조)
    - **RT09 LM**:
        - **추가 데이터**: CHIL 및 웹 데이터
        - **컷오프 변경**: 4그램의 최소 카운트를 4에서 3으로 설정
    - **RNN LM**:
        - **훈련 데이터**: 회의 전사 및 Switchboard 코퍼스 (총 5.4M 단어)
        - **제한 사항**: RNN 훈련이 많은 데이터로는 시간 소모가 큼
4. 성능 결과
    - WER로 판단해보면, 단일 RNN보다 여러개의 RNN 모델과 LM 모델을 결합하면 동적인 환경에서 더 효과적인 결과를 얻을 수 있음

## 5. Conclusion

---

1. 성능 비교시, 순환신경망  RNN 기반 모델이 백오프 모델에 비해 매우 뛰어난 성능
    - RNN >>> 최신 Back-off 모델 (with. 더 많은 데이터로 학습)
    - WSJ test 결과,
        - (동일 데이터) RNN vs Back off 모델 : 약 18%의 오류율 감소
        - (백오프만 5배 많은 데이터 학습) RNN vs Back off 모델 : 약 12%의 오류율 감소
    - NIST RT05 test 결과,
        - (단 540만 단어의 도메인 내 데이터) RNN vs (수백 배 더많은 데이터로 훈련)Back off 모델 : 오류율 감소
            
            → 더 나은 성능을 보임
            

1. insight
    - 다른 방식의 언어 모델링 필요
    - 데이터의 양이 많다고 더 좋은 성능을 보이는 것은 아님

1. 표2 해석
    
    ![[표 2]](RNN%20%E2%AD%90%20a90b26acde5843f28d0e071d435dfec5/Untitled%201.png)
    
    [표 2]
    
    - **PPL**  퍼플렉시티
        - 이는 유사한 데이터 셋으로 측정된 것 중 가장 큰 값을 넣어둔 것
        - Online learning의 중요한 효과가 보임
            - 동적 모델로 데이터를 순차적으로 학습하며 새로운 데이터를 얻을 때마다 즉시 학습을 반영
            - 음성 인식의 맥락에서 비지도 학습 언어모델 기법과 유사함
    - **WER** 단어오류율 → 음성인식의 정확도
        - 약간만 영향을 받아 테스트 데이터의 올바른 순서를 필요로 함
        - ~~데이터압축~~, 캐시와 유사한 정보 및 트리거와 유사한 정보를 자연스럽게 얻을 수 있는 방법을 제공하는 온라인 학습은 더 연구되어야 함
        
        →  언어를 실제로 학습할 수 있는 모델을 구축하려면 
        
        새로운 정보를 획득하는 것은 중요하기에 온라인 학습 필수적
        

1. 역전파(BPTT) 알고리즘 → 가중치 업데이트를 위해 사용
    - 역전파 알고리즘과 관련한 추가적인 개선 : LSTM
    - 단순한 RNN이 실제로 긴 문맥 정보를 포착할 수는 없음
    - 캐시모델은 여전히 역전파로 훈련된 온라인 모델에도 보완적인 정보 제공

🔼 위와 같은 결과로 다른 분야인 기계학습, 데이터 압축, 인지 과학 연구와도 밀접하게 연결됨

---

## ➕ 참고

- dsba 유튜브
    
    [[CS224d] 8. Recurrent neural network -조수현](https://www.youtube.com/watch?v=7USL-Ws3zVs&t=85s)
    
    [[DSBA]CS224N-06.Language Models and Recurrent Neural Network](https://www.youtube.com/watch?v=OyCZvSDxMAk)
    
- 코드 구현
    
    [RNN 코드구현](https://velog.io/@cbk6557/RNN-코드구현)
    
- 논문 리뷰 블로그
    
    [[NLP] Recurrent Neural Networks & Language Model](https://velog.io/@kyungmin1029/NLP-Recurrent-Neural-Networks-Language-Model)
    
- 코드 git
    
    [GitHub - gdiakidis/Recurrent-Neural-Network-from-Scratch-for-Language-Modelling](https://github.com/gdiakidis/Recurrent-Neural-Network-from-Scratch-for-Language-Modelling)
    
    - 유사 데이터로 태깅하는 작업
        
        [https://github.com/Shakya2485/POS-tagging-on-WSJ-corpus/blob/main/POS_tagging.ipynb](https://github.com/Shakya2485/POS-tagging-on-WSJ-corpus/blob/main/POS_tagging.ipynb)
        
    - 구조기반 코드
        
        [파이썬으로 기초 RNN 구현하기](https://hi-lu.tistory.com/entry/파이썬으로-기초-RNN-구현하기)