# BART ⭐

---

<aside>
💡 **BART**
노이즈가 있는 텍스트를 복원하기 위해 **양방향 인코더**와 **자동회귀 디코더**를 사용하는 Transformer 기반의 자연어 처리 모델

</aside>

![bart-simpson.gif](BART%20%E2%AD%90%20a0532ac33d5e43c8b0757b1b6e317a31/bart-simpson.gif)

---

## 0. Abstract

---

> **Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension**
> 
> - 자연어 생성, 번역 및 이해를 위한 시퀀스 간 노이즈 제거 사전 학습
> - Seq2seq 모델 사전학습을 위한 노이즈 제거 오토인코더 BART

## 1. Introduction

---

NLP에서의 자기지도학습 방법 👍🏻

- ex. word2vec, ELMo, BERT, XLNet, Roberta
- 가장 성공적인 방식으로는 마스킹된 언어모델 MLM 의 변형

→ 단점이 **특정 end task에 집중해 활용성이 떨어지는 문제가 발생**함

- 무작위로 선택된 일부 단어가 마스킹된 토큰의 분포를 복원하도록 훈련된 **디노이징 오토인코더** denoising autoencoder

**BART** Bidirectional and Auto-Regressive Transformers

- 양방향 및 오토회귀 프랜스포머를 결합한 모델을 사전훈련 시킨 모델
    - 사전 훈련은 두가지 단계로 진행
        - (1) 텍스트가 임의의 노이즈 함수로 변형
        - (2) 시퀀스 투 시퀀스 모델이 원래 텍스트를 복원
- Seq2Seq 모델로 구축된 디노이징 오토인코더로 매우 넓은 범위의 최종 작업에 적용
- 표준 트랜스포머 기반 신경 기계 번역 아키텍처를 사용
    
    ![스크린샷 2024-09-14 오전 7.08.43.png](BART%20%E2%AD%90%20a0532ac33d5e43c8b0757b1b6e317a31/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2024-09-14_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%258C%25E1%2585%25A5%25E1%2586%25AB_7.08.43.png)
    
    - 이는 그 단순함에도 불구하고 양방향 인코더 덕분에 BERT를, 좌에서 우로의 디코더 덕분에 GPT를, 그리고 더 최근의 여러 사전 훈련 방식들을 일반화
        
        ![[https://www.geeksforgeeks.org/bart-model-for-text-auto-completion-in-nlp/](https://www.geeksforgeeks.org/bart-model-for-text-auto-completion-in-nlp/)](BART%20%E2%AD%90%20a0532ac33d5e43c8b0757b1b6e317a31/image.png)
        
        [https://www.geeksforgeeks.org/bart-model-for-text-auto-completion-in-nlp/](https://www.geeksforgeeks.org/bart-model-for-text-auto-completion-in-nlp/)
        
        - **`BERT`**
            - Transformer - encoder(Bidirectional Encoder) 만 사용
            - MLM
            - masked Token을 예측하는 데에 사용되어 generation task에서 사용이 어려움
        - **`GPT`**
            - Transformer - decoder(Autoregressive Decoder)
            - 다음 토큰을 예측해 gerneration에 사용 가능하지만 bidirectional 정보를 얻지 못함
            - **자기회귀 특성**: BART의 디코더는 GPT와 마찬가지로 자기회귀 방식으로 작동
                - 이는 시퀀스의 각 단계에서 이전 단계를 바탕으로 다음 단어를 예측하는 방식
                - 이러한 특징은 BART가 텍스트 생성 작업에서 자연스럽고 일관된 출력을 생성
- 노이즈 처리에 유연성
    - 원본 텍스트에 임의의 변형을 적용할 수 있으며 길이를 변경하는 것도 가능
    - 원본 문장의 순서를 무작위로 섞는 것과 새로운 인필링(in-filling) 방식이 가장 좋은 성능을 보인다는 것을 발견
        - 인필링 방식 : 임의의 길이의 텍스트 스팬이 하나의 마스크 토큰으로 대체

## 2. Model Architecture

---

<aside>
💡

**BART** Bidirectional and Auto-Regressive Transformers

- 문장을 손상시킨 후, reconstruction loss(디코더의 출력과 원본 문서 간의)를 최적화하면서 학습
- ReLU 활성화 함수를 GeLU(Hendrycks & Gimpel, 2016)로 수정
- 특정 노이즈에 맞게 조정된 기존 denosing auto-encoder와 달리 어떤 종류의 문장 손상이든 적용
    - output 개수가 정해져 있지 않고, auto-regressive하게 생성되기 때문

---

🔽 다섯가지의 노이즈 기법을 적용해 사전학습 방법

![스크린샷 2024-09-14 오전 7.35.23.png](BART%20%E2%AD%90%20a0532ac33d5e43c8b0757b1b6e317a31/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2024-09-14_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%258C%25E1%2585%25A5%25E1%2586%25AB_7.35.23.png)

- Token Masking
    - 입력 문장에서 임의의 단어 하나를 마스킹하여 입력 문장을 훼손시키고, 디코더가 원래 문장을 정확하게 예측, 복원하도록 학습
- Token Deletion
    - 입력 문장에서 임의로 token을 제거하고 어느 자리에 token이 유실되었는지 결정
    - 토큰의 위치 정보가 주어지지 않은 채로 원본 문장을 복원
- **Text Infilling**
    - 입력 문장에서 하나 이상의 연속된 단어를 마스킹하여 입력 문장을 훼손시키고, 디코더가 원래 문장을 정확하게 예측, 복원하도록 학습하는 방식
    - 연속된 여러 단어를 정확히 예측
    - 많은 텍스트 구간이 샘플링되며 구간 길이는 포아송분포3에서 도출
- Sentence Permutation
    - 여러 문장으로 구성된 텍스트의 문장 순서를 무작위로 섞고, 디코더가 원래 순서의 문장들을 정확하게 예측, 복원하도록 학습하는 방식
- Document Rotation
    - 전체 문서의 일부를 잘라내어 그 부분을 문서의 시작으로 설정하는 방식으로 디코더는 이렇게 재배열된 문서를 원래의 순서대로 복원하도록 학습

---

🔽 4가지의 fine-tuning 기법 제시

- Sentence Classification Tasks
    - 동일한 입력이 인코더와 디코더에 입력되며, 마지막 디코더 토큰의 최종 히든 상태가 새로운 다중 클래스 선형 분류기로 전달
- Token Classification Tasks
    - 문서를 인코더와 디코더에 입력하고, 디코더의 최상위 히든 상태를 각 단어의 표현으로 사용하여 토큰을 분류
- Sequence Generation Tasks
    - 추상적 질문 응답이나 요약 같은 시퀀스 생성 작업에 파인튜닝
- Machine Translation Tasks
    - BART 전체를 하나의 사전 훈련된 디코더로 사용해 기계 번역 성능을 향상
</aside>

## 3.  Experiments

---

### Task

- **SQuAD**(Rajpurkar et al., 2016)
    - 위키백과 단락에 대한 추출 데이터셋
    - 질문 답변 작업을 위해 사용
- **MNLI**(Williams et al., 2017)
    - 한 문장이 다른 문장을 포함하는지 여부를 예측하는 이중 텍스트 분류 작업
    - 미세 조정된 모델은 두 문장을 EOS 토큰과 함께 연결하여 인코더와 디코더에 전달하며, EOS 토큰의 표현을 사용하여 문장 관계를 분류
- **ELI5**(Fan et al., 2019)
    - 장문형 추상적 질문 응답 데이터셋
    - 질문과 지원 문서의 연결을 조건으로 답변을 생성
- **XSum**(Narayan et al., 2018)
    - 추상적 뉴스 요약 데이터셋으로, 매우 추상적인 요약
- **ConvAI2**(Dinan et al., 2019)
    - 컨텍스트와 페르소나를 조건으로 대화 응답을 생성
- **CNN/DM**(Hermann et al., 2015)
    - 뉴스 요약 데이터셋입니다. 여기서 요약은 소스 문장과 밀접하게 연관

![스크린샷 2024-09-14 오전 7.24.07.png](BART%20%E2%AD%90%20a0532ac33d5e43c8b0757b1b6e317a31/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2024-09-14_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%258C%25E1%2585%25A5%25E1%2586%25AB_7.24.07.png)

**BART Base** - 인코더와 디코더에 6개의 층 사용

- 사전학습 방법의 효율성은 작업에 따라 크게 달라짐을 보임
    - 간단한 언어 모델은 최상의 ELI5 성능에 달하지만 SQUAD 달성
- 양방향 인코더는 SQuAD에 매우 중요

![스크린샷 2024-09-14 오전 7.24.32.png](BART%20%E2%AD%90%20a0532ac33d5e43c8b0757b1b6e317a31/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2024-09-14_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%258C%25E1%2585%25A5%25E1%2586%25AB_7.24.32.png)

**BART Large** - 인코더와 디코더에 12개의 층 사용

- RoBERTa 및 XLNet과 유사한 성능
- BART의 단방향 인코더 레이어가 차별적인 성능을 저하시키지 않는다는 점을 시사

![스크린샷 2024-09-14 오전 7.24.59.png](BART%20%E2%AD%90%20a0532ac33d5e43c8b0757b1b6e317a31/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2024-09-14_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%258C%25E1%2585%25A5%25E1%2586%25AB_7.24.59.png)

 텍스트 생성 작업에서 요약실험 결과에서도 뛰어난 성능

![스크린샷 2024-09-14 오전 7.25.45.png](BART%20%E2%AD%90%20a0532ac33d5e43c8b0757b1b6e317a31/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2024-09-14_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%258C%25E1%2585%25A5%25E1%2586%25AB_7.25.45.png)

CONVAI2(Dinan et al., 2019)에서 대화 응답 생성 성능을 평가에서도 뛰어난 성능

![스크린샷 2024-09-14 오전 7.25.30.png](BART%20%E2%AD%90%20a0532ac33d5e43c8b0757b1b6e317a31/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2024-09-14_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%258C%25E1%2585%25A5%25E1%2586%25AB_7.25.30.png)

모델이 장문의 자유 형식 답변을 생성할 수 있는 능력을 테스트하기 위해 최근에 제안된 ELI5 데이터셋을 사용

→ 성능은 좋지만 아직 구체적인 답변을 하지는 않음 

![스크린샷 2024-09-14 오전 7.26.16.png](BART%20%E2%AD%90%20a0532ac33d5e43c8b0757b1b6e317a31/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2024-09-14_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%258C%25E1%2585%25A5%25E1%2586%25AB_7.26.16.png)

WMT16 루마니아어-영어 데이터셋에서도 성능을 평가진행 

- 6개의 레이어로 구성된 트랜스포머 소스 인코더를 사용해 루마니아어를 BART가 영어로 디노이즈할 수 있는 표현으로 변환
- 연구에서는 추가적인 규제(regularization) 기법을 탐구해 과적합 예방 필요

## 4. Conclusion

---

- BART는 손상된 문서를 기존 문서로 매핑하는 것으로 학습하는 사전학습 방법론을 이용한 모델
- Discriminative Task에서 RoBERTa와 비슷한 성능을 보임
- 여러 텍스트 생성 태스크에서 SOTA 달성

## 🎹 코드 리뷰

---

## ➕ 참고

---

[20-02. BART(Bidirectional Auto-Regressive Transformers)](https://wikidocs.net/256572)

[NLP 논문 리뷰📎 BART(2019) : Denoising Sequence-to-Sequence Pre-training for Natural Language Generation,](https://www.youtube.com/watch?v=epKnADvuIlo&t=55s)

[https://github.com/ljm565/chatbot-BART](https://github.com/ljm565/chatbot-BART)