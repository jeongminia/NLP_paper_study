# NLP_paper_study
- ÏùºÏ†ï : 2024.07.08 ~ 
- Îß§Ï£º ÏùºÏöîÏùº Ïò§Ï†Ñ 10Ïãú
- ÎÖºÎ¨∏ Î¶¨Îî© ÌõÑ Ìï¥Îãπ Î™®Îç∏ Î∂ÄÎ∂Ñ ÏΩîÎìú Î¶¨Î∑∞ ÌõÑ ÏóÖÎ°úÎìú

## üìÉ Paper List
||model|paper|date|code|note|
|:--:|:--:|:-----:|:---:|:---:|:---:|
|01|RNN|[Recurrent neural network based language model](https://arxiv.org/pdf/1409.3215.pdf)|2024.07.14|[code](https://github.com/jeongmin1016/Paper_study/blob/main/code/RNN_0714.ipynb)|[note](https://velog.io/@jeongminii/paper-review-Recurrent-neural-network-based-language-model)|  
|02|LSTM|[Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling](https://static.googleusercontent.com/media/research.google.com/ko//pubs/archive/43905.pdf)|2024.07.14|[code]( )|[note](https://velog.io/@jeongminii/paper-review-Long-Short-Term-Memory-Recurrent-Neural-Network-Architectures-for-Large-Scale-Acoustic-Modeling)|
|03|GRU|[Learning Phrase Representations using RNN Encoder‚ÄìDecoder for Statistical Machine Translation](https://arxiv.org/pdf/1406.1078)|2024.07.21|[code](https://github.com/jeongmin1016/Paper_study/blob/main/code/GRU_0721.ipynb)|[note](https://velog.io/@jeongminii/paper-review-Learning-Phrase-Representations-using-RNN-EncoderDecoder-for-Statistical-Machine-Translation)|
|04|Seq2seq|[Sequence to Sequence Learning with Neural Networks](https://arxiv.org/pdf/1409.3215)|2024.07.21|[code]( )|[note]( )|
|05|Attention|[Neural machine translation by jointly learning to align and translate](https://arxiv.org/pdf/1409.0473)|2024.07.28|[code]()|[note](https://velog.io/@jeongminii/paper-review-Neural-machine-translation-by-jointly-learning-to-align-and-translate)|
|06|Transformer|[Attention is All you need](https://static.googleusercontent.com/media/research.google.com/ko//pubs/archive/43905.pdf)|2024.07.28|[code]( )|[note]( )|
|07|Word2Vec|[Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781)|2024.08.04|[code](https://github.com/jeongmin1016/Paper_study/blob/main/code/Word2Vec_0804.ipynb)|[note]( )|
|08|GloVe|[GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf)|2024.08.04|[code]()|[note]( )|
|09|FastText|[Enriching Word Vectors with Subword Information](https://aclanthology.org/Q17-1010.pdf)|2024.08.11|[code]()|[note]( )|
|10|ELMo|[Deep contextualized word representations](https://arxiv.org/pdf/1802.05365)|2024.08.11|[code]()|[note]( )|
|11|BERT|[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805)|2024.08.18|[code]()|[note]( )|
|12|GPT-3|[Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165)|2024.08.18|[code]()|[note]( )|
|13|LLaMA|[LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/pdf/2302.13971v1)|2024.08.24|[code]()|[note]( )|
|14|LoRA|[LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2106.09685)|2024.08.24|[code]()|[note]( )|
|15|BART|[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/pdf/1910.13461)|2024.09.14|[code](https://github.com/jeongminia/NLP_paper_study/blob/main/code/BART_0914.ipynb)|[note]( )|
|16|GNN|[A Comprehensive Survey on Graph Neural Networks](https://arxiv.org/pdf/1901.00596)|2024.09.14|[code]()|[note]( )|
|17|T5|[]()|2024.09.22|[code]()|[note]( )|
|18|Time Series Transformer|[Are Transformers Effective for Time Series Forecasting?](https://arxiv.org/pdf/2205.13504)|2024.09.22|[code]()|[note]( )|

----
#### Architecture
- [RNN Í∏∞Î∞ò Î™®Îç∏ ÎπÑÍµê](https://github.com/jeongmin1016/NLP_paper_study/blob/main/note/models_.md)
