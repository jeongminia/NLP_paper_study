# NLP_paper_study
- ì¼ì • : 2024.07.08 ~ 
- ë§¤ì£¼ ì¼ìš”ì¼ ì˜¤ì „ 10ì‹œ
- ë…¼ë¬¸ ë¦¬ë”© í›„ í•´ë‹¹ ëª¨ë¸ ë¶€ë¶„ ì½”ë“œ ë¦¬ë·° í›„ ì—…ë¡œë“œ

## ğŸ“ƒ Paper List

|  | model | paper | date | code | note | ğŸŒšÂ ğŸŒ |
| --- | --- | --- | --- | --- | --- | --- |
| 01 | RNN | [Recurrent neural network based language model](https://arxiv.org/pdf/1409.3215.pdf) | 2024.07.14 | [code](https://github.com/jeongmin1016/Paper_study/blob/main/code/RNN_0714.ipynb) | [note](https://velog.io/@jeongminii/paper-review-Recurrent-neural-network-based-language-model) | ğŸŒ |
| 02 | LSTM | [Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling](https://static.googleusercontent.com/media/research.google.com/ko//pubs/archive/43905.pdf) | 2024.07.14 | code | [note](https://velog.io/@jeongminii/paper-review-Long-Short-Term-Memory-Recurrent-Neural-Network-Architectures-for-Large-Scale-Acoustic-Modeling) | ğŸŒš |
| 03 | GRU | [Learning Phrase Representations using RNN Encoderâ€“Decoder for Statistical Machine Translation](https://arxiv.org/pdf/1406.1078) | 2024.07.21 | [code](https://github.com/jeongmin1016/Paper_study/blob/main/code/GRU_0721.ipynb) | [note](https://velog.io/@jeongminii/paper-review-Learning-Phrase-Representations-using-RNN-EncoderDecoder-for-Statistical-Machine-Translation) | ğŸŒ |
| 04 | Seq2seq | [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/pdf/1409.3215) | 2024.07.21 | code | note | ğŸŒš |
| 05 | Attention | [Neural machine translation by jointly learning to align and translate](https://arxiv.org/pdf/1409.0473) | 2024.07.28 | code | [note](https://velog.io/@jeongminii/paper-review-Neural-machine-translation-by-jointly-learning-to-align-and-translate) | ğŸŒ |
| 06 | Transformer | [Attention is All you need](https://static.googleusercontent.com/media/research.google.com/ko//pubs/archive/43905.pdf) | 2024.07.28 | code | note | ğŸŒš |
| 07 | Word2Vec | [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781) | 2024.08.04 | [code](https://github.com/jeongmin1016/Paper_study/blob/main/code/Word2Vec_0804.ipynb) | note | ğŸŒ |
| 08 | GloVe | [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf) | 2024.08.04 | code | note | ğŸŒš |
| 09 | FastText | [Enriching Word Vectors with Subword Information](https://aclanthology.org/Q17-1010.pdf) | 2024.08.11 | code | note | ğŸŒš |
| 10 | ELMo | [Deep contextualized word representations](https://arxiv.org/pdf/1802.05365) | 2024.08.11 | code | note | ğŸŒ |
| 11 | BERT | [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805) | 2024.08.18 | code | note | ğŸŒ |
| 12 | GPT-3 | [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165) | 2024.08.18 | code | note | ğŸŒš |
| 13 | LLaMA | [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/pdf/2302.13971v1) | 2024.08.24 | code | note | ğŸŒš |
| 14 | LoRA | [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2106.09685) | 2024.08.24 | code | note | ğŸŒ |
| 15 | BART | [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/pdf/1910.13461) | 2024.09.14 | [code](https://github.com/jeongminia/NLP_paper_study/blob/main/code/BART_0914.ipynb) | note | ğŸŒ |
| 16 | GNN | [A Comprehensive Survey on Graph Neural Networks](https://arxiv.org/pdf/1901.00596) | 2024.09.14 | code | note | ğŸŒš |
| 17 | T5 | [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683) | 2024.09.22 | code | note | ğŸŒš |
| 18 | Time Series Transformer | [Are Transformers Effective for Time Series Forecasting?](https://arxiv.org/pdf/2205.13504) | 2024.09.22 | code | note | ğŸŒ |
| 19 | FiLM | [FiLM: Visual Reasoning with a General Conditioning Layer](https://arxiv.org/pdf/1709.07871) | 2024.09.29 | code | note | ğŸŒš |
| 20 | QLoRA | [QLORA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/pdf/2305.14314) | 2024.09.29 | code | note | ğŸŒ |
| 21 | RoBERTa | [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/pdf/1907.11692) | 2024.10.05 | code | note | ğŸŒ |
| 22 | WaveNet | [WAVENET: A GENERATIVE MODEL FOR RAW AUDIO](https://arxiv.org/pdf/1609.03499) | 2024.10.05 | code | note | ğŸŒš |
| 23 | MiniLM | [MINILM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers](https://arxiv.org/abs/2002.10957) |  | code | note |  |
| 24 | GCN | [SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS](https://arxiv.org/pdf/1609.02907) |  | code | note |  |


----
#### Architecture
- [RNN ê¸°ë°˜ ëª¨ë¸ ë¹„êµ](https://github.com/jeongmin1016/NLP_paper_study/blob/main/note/models_.md)
