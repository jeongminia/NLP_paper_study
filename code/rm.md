# RNN

---

💡 ***RNN*  순환신경망
R**ecurrent **n**eural **n**etwork based language model

> 특수 용어
> - PPL : 언어모델 성능 지표
> - WER : 단어 오류율로 음식 음성인식 정확도로도 볼 수 있음

---

## 0. Abstract

---

- 음성 인식에 적용할 수 있는 새로운 순환 신경망 기반 언어모델
- 여러 RNN LMs RNN 기반 언어 모델을 함께 사용하면 최신 Backoff LM에 비해 perplexity를 약 50% 감소할 수 있어 RNN기반 언어모델 성능이 더 좋음

## 1. Introduction

---

### RNN 이전

- 주어진 문맥 내 텍스트 데이터의 다음 단어를 예측하는 것을 목표로 하며 **통계적 언어 모델링 LM**
    - 모델링 이전에 몇가지 각각의 가정들을 보유
- **cache models** 과 **class-based models** 의 도입으로 상당한 개선이 이뤄짐

### 한계

- 고급 언어 모델링 기술이 **실제 응용**에서 성공을 거두는 데 어려움있으며 실용성이 낮음
- 시퀀스 데이터처리, 장기 의존성 학습, 동적 길이 입력처리 등과 같은 문제에 부딪힘
    
    ⇒ 실제 응용에서는 데이터가 많을 때 간단한 모델들보다도 더 큰 차이를 만들지 못하는 경우가 많음
    

## 2. Model description

---

- 밑딥2 : `RNN`
    
    
    ![스크린샷 2024-04-05 오후 4.49.47.png](RNN%20cc05f6b8cf5f42bd83ebab042d171abf/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2024-04-05_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_4.49.47.png)
    
    ![fig 5-8.png](RNN%20cc05f6b8cf5f42bd83ebab042d171abf/fig_5-8.png)
    
    - 은닉 정보를 업데이트해가며 연속된 입력 처리하며 데이터가 순환하면서 정보가 끊임없이 갱신
    - 순환 경로를 따라 데이터는 끊임없이 순환하며 과거의 정보를 기억하는 동시에 최신 데이터로 갱신
        - $X_t$
            - $(x_0, x_1, …, x_t, …)$ 입력값이며 시계열 데이터에 해당
                - $x_t$
                    
                    각 단어의 분산 표현
                    
            - 각각의 벡터가 모여 행렬
        - $h_t$
            - $(h_0, h_1, …, h_t, …)$ 입력에 대응되는 출력값
    - 시계열 데이터는 시간 방향으로 데이터 나열
    - t 시점의 RNN 계층은 그 계층의 입력 $x_t$(행벡터)과 1개의 전의 RNN계층으로부터의 출력
        
        $h_{t-1}$ (행벡터)을 받음
        
    - $h_t = tanh(h_{t-1}W_h + x_tW_x+b)$

---

### RNN

**구조**

- Elman network 구조 or 가장 단순한 버전의 순환 신경망 SRNN 을 사용
    
    → RNN의 가장 기본적인 형태로 구현 및 훈련이 매우 쉬움
    
- $x$ 입력층 / $s$ 은닉층 / $y$ 출력층
- $x(t)$는 시간 t에서의 네트워크 입 벡터로 현재 단어를 나타내는 벡터 w와 시간 t-1에서 context layer의 뉴런 출력의 연결로 형성

**수식**

- **입력벡터**  $x(t)=w(t)+s(t−1)$
- **은닉층** 상태 계산
    - 입력 벡터의 각 요소와 가중치의 곱을 모두 더한 후 sigmoid 함수를 적용해 은닉층 상태 계산
        - 시그모이드 활성화 함수 : 출력값 0 ~ 1
- **출력층** 상태 계산
    - 은닉층의 각 요소와 가중치의 곱을 모두 더한 후 softmax 활성화 함수를 적용해 출력층 상태 계산
        - 소프트맥스 활성화함수 : 출력값 0 ~ 1로 모든 출력값의 합이 1이 되도록 해 다중 클래스 분류에서 사용

**특징** 

- 오류 벡터기반 가중치 업데이트
    - 오류벡터 : 각 훈련 단계에서 cross entropy 기준으로 오류 벡터를 계산하고 표준 역전파 알고리즘을 사용해 가중치 업데이트
- 다이나믹 모델
    - 고정된 학습률 0.01을 사용하며 훈련
        - ex. dog cat → dog 확률이 증가하면 cat의 확률도 증가
    - 새로운 도메인에 자동적으로 적용할 수 있음 ⭐
    - dynamic RNN (deprecated) vs static RNN
        - 문장의 길이(seq_len)만큼 recurrent가 일어나지만 정적은 고정
        - 별도로 각 문장마다 길이가 얼마인지를 명시해줘야 하지만 정적은 별도의 패딩 필요

🔽 실험에 앞서

- 훈련 데이터에서 자주 등장하지 않는 단어들을 단순화하고 처리 효율성을 높이는 방법으로는 임곗값 이하로 자주 등장하는 모든 단어들을 하나의 특수한 토큰으로 병합

## 3. WSJ experiments - 텍스트

---

![스크린샷 2024-07-14 오전 8.52.39.png](RNN%20cc05f6b8cf5f42bd83ebab042d171abf/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2024-07-14_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%258C%25E1%2585%25A5%25E1%2586%25AB_8.52.39.png)

- **모델 결합 방법** : RNN LM에 0.75, backoff LM에 0.25의 가중치를 주어 선형 보간
    
    → 퍼플렉서티(PPL)와 단어 오류율(WER)을 평가
    
- **결합 모델** : 다양한 아키텍처의 여러 RNN LM의 출력을 선형 보간
    - **복잡한 모델일수록 성능 향상**: RNN의 은닉층 크기와 희귀 토큰 임계값에 따라 성능이 향상
    - 모델 결합 효과 : 단일 RNN모델보다 더 나은 성능을 보임
- **Oracle WER**: RNN+KN 모델이 RNN 모델보다 더 낮은 WER을 보임.
- **Perplexity**: 일반적으로 RNN+KN 모델이 RNN 모델보다 퍼플렉서티가 낮아 더 나은 성능을 보임

⏩ 모델 결합 방법이 언어모델 성능 향상에 중요 역할을 함, 복잡한 모델 구성과 최적화된 가중치 조정이 성능평가에 영향을 크게 미침

## 4. NIST RT05 experiments - 음성

---

- **문제 상황** : 앞선 실험에서 사용된 모델이 최신 상태가 아니였으며, 사용된 데이터가 충분하지 않음
- **데이터 출처** : NIST RT05 평가에 사용된 AMI 시스템의 라티스를 통해 115시간의 음향
- **실험 설정**
    - 교차 단어 결합 상태를 기반으로 한 특정 유형의 음향 모델과 다양한 데이터로 훈련된 언어모델 사용
    - 최신 시스템에서도 의미 있는 개선을 입증하기 위해 NIST RT05 평가를 통해 성능 비교
- **성능 결과**
    - WER로 판단해보면, 단일 RNN보다 여러개의 RNN 모델과 LM 모델을 결합하면 동적인 환경에서 더 효과적인 결과를 얻을 수 있음

⏩ 단일 RNN 모델보다 여러 RNN 모델과 LM 모델을 결합하는 것이 동적인 환경에서 더 효과적인 결과를 도출할 수 있음. 다양한 모델과 데이터 소스를 결합하면서 음성 인식 시스템의 정확성과 신뢰성 향상 가능

## 5. Conclusion

---

1. 성능 비교시, 순환신경망  RNN 기반 모델이 백오프 모델에 비해 매우 뛰어난 성능
2. `insight`
    - 다른 방식의 언어 모델링 필요
    - 데이터의 양이 많다고 더 좋은 성능을 보이는 것은 아님
3. Online learing의 중요효과
    - 동적 모델로 데이터를 순차적으로 학습하며 새로운 데이터를 얻을 때마다 즉시 학습을 반영
4. **WER** 단어오류율
    - 약간만 영향을 받아 테스트 데이터의 올바른 순서를 필요로 함
5. 역전파(BPTT) 알고리즘 → 가중치 업데이트를 위해 사용
    - RNN의 가장 단순한 형태를 바닐라 RNN로 이는 시점이 길어질수록 앞의 정보가 뒤로 충분히 전달되지 못하는 현상이 발생
    - 단순한 RNN이 실제로 긴 문맥 정보를 포착할 수는 없음 ⇒ `LSTM`, `GRU`

🔼 위와 같은 결과로 다른 분야인 기계학습, 데이터 압축, 인지 과학 연구와도 밀접하게 연결됨

