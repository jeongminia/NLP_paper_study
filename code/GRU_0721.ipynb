{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["jxjzjnUbrP9D"],"gpuType":"T4","authorship_tag":"ABX9TyPHQJ+Nz0qLsTkabfxbe9Wg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["GRU를 이용한 IMDB 리뷰 분류"],"metadata":{"id":"WsfP8d6apbkU"}},{"cell_type":"markdown","source":["# 0. 라이브러리 설치"],"metadata":{"id":"p7TYyaR8rR67"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"7e9s8b_io3n4","executionInfo":{"status":"ok","timestamp":1721521192910,"user_tz":-540,"elapsed":11578,"user":{"displayName":"Jeongmin Ahn","userId":"13913237267246789835"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import nltk\n","import torch\n","import urllib.request\n","from tqdm import tqdm\n","from collections import Counter\n","from nltk.tokenize import word_tokenize\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"],"metadata":{"id":"OfpxO_rprbdQ","executionInfo":{"status":"ok","timestamp":1721521521324,"user_tz":-540,"elapsed":470,"user":{"displayName":"Jeongmin Ahn","userId":"13913237267246789835"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BmXECfKMpSlK","executionInfo":{"status":"ok","timestamp":1721521194284,"user_tz":-540,"elapsed":1377,"user":{"displayName":"Jeongmin Ahn","userId":"13913237267246789835"}},"outputId":"50e97f77-6772-4f0c-a812-244100a54a65"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","source":["# 1. 데이터 불러오기"],"metadata":{"id":"jxjzjnUbrP9D"}},{"cell_type":"code","source":["urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/pytorch-nlp-tutorial/main/10.%20RNN%20Text%20Classification/dataset/IMDB%20Dataset.csv\", filename=\"IMDB Dataset.csv\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YhCx8KnRpUkE","executionInfo":{"status":"ok","timestamp":1721521201209,"user_tz":-540,"elapsed":6929,"user":{"displayName":"Jeongmin Ahn","userId":"13913237267246789835"}},"outputId":"a33dd1bf-4023-4be6-d6ce-b1f915954437"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('IMDB Dataset.csv', <http.client.HTTPMessage at 0x7e6b9eb426e0>)"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["df = pd.read_csv('IMDB Dataset.csv')\n","df['sentiment'] = df['sentiment'].replace(['positive','negative'],[1, 0]) # 긍정/부정 레이블 치환\n","df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"pjhvWYkepWoe","executionInfo":{"status":"ok","timestamp":1721521201867,"user_tz":-540,"elapsed":673,"user":{"displayName":"Jeongmin Ahn","userId":"13913237267246789835"}},"outputId":"053f27cc-5f3a-42c1-a5f1-22ee05e48836"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                              review  sentiment\n","0  One of the other reviewers has mentioned that ...          1\n","1  A wonderful little production. <br /><br />The...          1\n","2  I thought this was a wonderful way to spend ti...          1\n","3  Basically there's a family where a little boy ...          0\n","4  Petter Mattei's \"Love in the Time of Money\" is...          1"],"text/html":["\n","  <div id=\"df-ea1e18bf-5717-4d9d-8ec6-5d887539c0c3\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>One of the other reviewers has mentioned that ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>I thought this was a wonderful way to spend ti...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Basically there's a family where a little boy ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ea1e18bf-5717-4d9d-8ec6-5d887539c0c3')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-ea1e18bf-5717-4d9d-8ec6-5d887539c0c3 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-ea1e18bf-5717-4d9d-8ec6-5d887539c0c3');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-73d8d43b-3c83-4380-9c46-a0d87a11b4b8\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-73d8d43b-3c83-4380-9c46-a0d87a11b4b8')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-73d8d43b-3c83-4380-9c46-a0d87a11b4b8 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 50000,\n  \"fields\": [\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 49582,\n        \"samples\": [\n          \"\\\"Soul Plane\\\" is a horrible attempt at comedy that only should appeal people with thick skulls, bloodshot eyes and furry pawns. <br /><br />The plot is not only incoherent but also non-existent, acting is mostly sub sub-par with a gang of highly moronic and dreadful characters thrown in for bad measure, jokes are often spotted miles ahead and almost never even a bit amusing. This movie lacks any structure and is full of racial stereotypes that must have seemed old even in the fifties, the only thing it really has going for it is some pretty ladies, but really, if you want that you can rent something from the \\\"Adult\\\" section. OK?<br /><br />I can hardly see anything here to recommend since you'll probably have a lot a better and productive time chasing rats with a sledgehammer or inventing waterproof teabags or whatever.<br /><br />2/10\",\n          \"Guest from the Future tells a fascinating story of time travel, friendship, battle of good and evil -- all with a small budget, child actors, and few special effects. Something for Spielberg and Lucas to learn from. ;) A sixth-grader Kolya \\\"Nick\\\" Gerasimov finds a time machine in the basement of a decrepit building and travels 100 years into the future. He discovers a near-perfect, utopian society where robots play guitars and write poetry, everyone is kind to each other and people enjoy everything technology has to offer. Alice is the daughter of a prominent scientist who invented a device called Mielophone that allows to read minds of humans and animals. The device can be put to both good and bad use, depending on whose hands it falls into. When two evil space pirates from Saturn who want to rule the universe attempt to steal Mielophone, it falls into the hands of 20th century school boy Nick. With the pirates hot on his tracks, he travels back to his time, followed by the pirates, and Alice. Chaos, confusion and funny situations follow as the luckless pirates try to blend in with the earthlings. Alice enrolls in the same school Nick goes to and demonstrates superhuman abilities in PE class. The catch is, Alice doesn't know what Nick looks like, while the pirates do. Also, the pirates are able to change their appearance and turn literally into anyone. (Hmm, I wonder if this is where James Cameron got the idea for Terminator...) Who gets to Nick -- and Mielophone -- first? Excellent plot, non-stop adventures, and great soundtrack. I wish Hollywood made kid movies like this one...\",\n          \"\\\"National Treasure\\\" (2004) is a thoroughly misguided hodge-podge of plot entanglements that borrow from nearly every cloak and dagger government conspiracy clich\\u00e9 that has ever been written. The film stars Nicholas Cage as Benjamin Franklin Gates (how precious is that, I ask you?); a seemingly normal fellow who, for no other reason than being of a lineage of like-minded misguided fortune hunters, decides to steal a 'national treasure' that has been hidden by the United States founding fathers. After a bit of subtext and background that plays laughably (unintentionally) like Indiana Jones meets The Patriot, the film degenerates into one misguided whimsy after another \\u0096 attempting to create a 'Stanley Goodspeed' regurgitation of Nicholas Cage and launch the whole convoluted mess forward with a series of high octane, but disconnected misadventures.<br /><br />The relevancy and logic to having George Washington and his motley crew of patriots burying a king's ransom someplace on native soil, and then, going through the meticulous plan of leaving clues scattered throughout U.S. currency art work, is something that director Jon Turteltaub never quite gets around to explaining. Couldn't Washington found better usage for such wealth during the start up of the country? Hence, we are left with a mystery built on top of an enigma that is already on shaky ground by the time Ben appoints himself the new custodian of this untold wealth. Ben's intentions are noble \\u0096 if confusing. He's set on protecting the treasure. For who and when?\\u0085your guess is as good as mine.<br /><br />But there are a few problems with Ben's crusade. First up, his friend, Ian Holmes (Sean Bean) decides that he can't wait for Ben to make up his mind about stealing the Declaration of Independence from the National Archives (oh, yeah \\u0096 brilliant idea!). Presumably, the back of that famous document holds the secret answer to the ultimate fortune. So Ian tries to kill Ben. The assassination attempt is, of course, unsuccessful, if overly melodramatic. It also affords Ben the opportunity to pick up, and pick on, the very sultry curator of the archives, Abigail Chase (Diane Kruger). She thinks Ben is clearly a nut \\u0096 at least at the beginning. But true to action/romance form, Abby's resolve melts quicker than you can say, \\\"is that the Hope Diamond?\\\" The film moves into full X-File-ish mode, as the FBI, mistakenly believing that Ben is behind the theft, retaliate in various benign ways that lead to a multi-layering of action sequences reminiscent of Mission Impossible meets The Fugitive. Honestly, don't those guys ever get 'intelligence' information that is correct? In the final analysis, \\\"National Treasure\\\" isn't great film making, so much as it's a patchwork rehash of tired old bits from other movies, woven together from scraps, the likes of which would make IL' Betsy Ross blush.<br /><br />The Buena Vista DVD delivers a far more generous treatment than this film is deserving of. The anamorphic widescreen picture exhibits a very smooth and finely detailed image with very rich colors, natural flesh tones, solid blacks and clean whites. The stylized image is also free of blemishes and digital enhancements. The audio is 5.1 and delivers a nice sonic boom to your side and rear speakers with intensity and realism. Extras include a host of promotional junket material that is rather deep and over the top in its explanation of how and why this film was made. If only, as an audience, we had had more clarification as to why Ben and co. were chasing after an illusive treasure, this might have been one good flick. Extras conclude with the theatrical trailer, audio commentary and deleted scenes. Not for the faint-hearted \\u0096 just the thick-headed.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["X_data = df['review']\n","y_data = df['sentiment']\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.5, random_state=0, stratify=y_data)\n","X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=.2, random_state=0, stratify=y_train)"],"metadata":{"id":"vpVszwAopZOg","executionInfo":{"status":"ok","timestamp":1721521201868,"user_tz":-540,"elapsed":4,"user":{"displayName":"Jeongmin Ahn","userId":"13913237267246789835"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["# 2. 데이터 전처리"],"metadata":{"id":"8PbrLqX3q2DX"}},{"cell_type":"code","source":["print('리뷰의 최대 길이 :',max(len(review) for review in encoded_X_train))\n","print('리뷰의 평균 길이 :',sum(map(len, encoded_X_train))/len(encoded_X_train))\n","plt.hist([len(review) for review in encoded_X_train], bins=50)\n","plt.xlabel('length of samples')\n","plt.ylabel('number of samples')\n","plt.show()\n","# 토큰화\n","def tokenize(sentences):\n","  tokenized_sentences = []\n","  for sent in tqdm(sentences):\n","    tokenized_sent = word_tokenize(sent)\n","    tokenized_sent = [word.lower() for word in tokenized_sent]\n","    tokenized_sentences.append(tokenized_sent)\n","  return tokenized_sentences\n","\n","tokenized_X_train = tokenize(X_train)\n","tokenized_X_valid = tokenize(X_valid)\n","tokenized_X_test = tokenize(X_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b8Y7Ob3Kp9bn","executionInfo":{"status":"ok","timestamp":1721521294282,"user_tz":-540,"elapsed":92418,"user":{"displayName":"Jeongmin Ahn","userId":"13913237267246789835"}},"outputId":"8e806fc2-7498-4716-c0d6-7c4efde65d94"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 20000/20000 [00:37<00:00, 539.85it/s]\n","100%|██████████| 5000/5000 [00:09<00:00, 503.58it/s]\n","100%|██████████| 25000/25000 [00:44<00:00, 561.95it/s]\n"]}]},{"cell_type":"code","source":["# 토큰화 된 훈련 데이터로부터 정수 인코딩을 진행하기 위한 단어 집합(Vocabulary)생성\n","word_list = []\n","for sent in tokenized_X_train:\n","    for word in sent:\n","      word_list.append(word)\n","\n","word_counts = Counter(word_list)\n","print('총 단어수 :', len(word_counts))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uDPpT6WWqEo6","executionInfo":{"status":"ok","timestamp":1721521348202,"user_tz":-540,"elapsed":1814,"user":{"displayName":"Jeongmin Ahn","userId":"13913237267246789835"}},"outputId":"1778f91f-e29c-49ae-95e2-01ff609d88e1"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["총 단어수 : 100586\n"]}]},{"cell_type":"code","source":["vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n","print('등장 빈도수 상위 10개 단어')\n","print(vocab[:10])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UQabnz1nqSRS","executionInfo":{"status":"ok","timestamp":1721521349862,"user_tz":-540,"elapsed":3,"user":{"displayName":"Jeongmin Ahn","userId":"13913237267246789835"}},"outputId":"58bc1630-c82f-44ec-a951-62e49c7951bc"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["등장 빈도수 상위 10개 단어\n","['the', ',', '.', 'a', 'and', 'of', 'to', 'is', '/', '>']\n"]}]},{"cell_type":"code","source":["threshold = 3\n","total_cnt = len(word_counts) # 단어의 수\n","rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n","total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n","rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n","\n","# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n","for key, value in word_counts.items():\n","    total_freq = total_freq + value\n","\n","    # 단어의 등장 빈도수가 threshold보다 작으면\n","    if(value < threshold):\n","        rare_cnt = rare_cnt + 1\n","        rare_freq = rare_freq + value\n","\n","print('단어 집합(vocabulary)의 크기 :',total_cnt)\n","print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n","print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n","print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TcJl8Wl0qXwy","executionInfo":{"status":"ok","timestamp":1721521351218,"user_tz":-540,"elapsed":1,"user":{"displayName":"Jeongmin Ahn","userId":"13913237267246789835"}},"outputId":"05e62d06-87e9-47bb-f58a-775507b1808a"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["단어 집합(vocabulary)의 크기 : 100586\n","등장 빈도가 2번 이하인 희귀 단어의 수: 61877\n","단어 집합에서 희귀 단어의 비율: 61.51651323245779\n","전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 1.3294254426463437\n"]}]},{"cell_type":"code","source":["# 전체 단어 개수 중 빈도수 2이하인 단어는 제거.\n","vocab_size = total_cnt - rare_cnt\n","vocab = vocab[:vocab_size]\n","print('단어 집합의 크기 :', len(vocab))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jQ_0EUeJqY5G","executionInfo":{"status":"ok","timestamp":1721521352699,"user_tz":-540,"elapsed":520,"user":{"displayName":"Jeongmin Ahn","userId":"13913237267246789835"}},"outputId":"070d4d92-b31d-4d08-827a-755882d49139"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["단어 집합의 크기 : 38709\n"]}]},{"cell_type":"code","source":["word_to_index = {}\n","word_to_index['<PAD>'] = 0\n","word_to_index['<UNK>'] = 1\n","\n","for index, word in enumerate(vocab) :\n","  word_to_index[word] = index + 2\n","\n","vocab_size = len(word_to_index)\n","print('패딩 토큰과 UNK 토큰을 고려한 단어 집합의 크기 :', vocab_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aafZ9g9wqbA1","executionInfo":{"status":"ok","timestamp":1721521352699,"user_tz":-540,"elapsed":3,"user":{"displayName":"Jeongmin Ahn","userId":"13913237267246789835"}},"outputId":"e097194b-e81d-4ffe-9c39-9ce0da7fe6fd"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["패딩 토큰과 UNK 토큰을 고려한 단어 집합의 크기 : 38711\n"]}]},{"cell_type":"code","source":["def texts_to_sequences(tokenized_X_data, word_to_index):\n","  encoded_X_data = []\n","  for sent in tokenized_X_data:\n","    index_sequences = []\n","    for word in sent:\n","      try:\n","          index_sequences.append(word_to_index[word])\n","      except KeyError:\n","          index_sequences.append(word_to_index['<UNK>'])\n","    encoded_X_data.append(index_sequences)\n","  return encoded_X_data\n","\n","encoded_X_train = texts_to_sequences(tokenized_X_train, word_to_index)\n","encoded_X_valid = texts_to_sequences(tokenized_X_valid, word_to_index)\n","encoded_X_test = texts_to_sequences(tokenized_X_test, word_to_index)\n"],"metadata":{"id":"Z_F1EtH_qa-Z","executionInfo":{"status":"ok","timestamp":1721521355389,"user_tz":-540,"elapsed":2691,"user":{"displayName":"Jeongmin Ahn","userId":"13913237267246789835"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["def below_threshold_len(max_len, nested_list):\n","  count = 0\n","  for sentence in nested_list:\n","    if(len(sentence) <= max_len):\n","        count = count + 1\n","  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (count / len(nested_list))*100))\n","\n","max_len = 500\n","below_threshold_len(max_len, encoded_X_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AsKpovEdqa59","executionInfo":{"status":"ok","timestamp":1721521420470,"user_tz":-540,"elapsed":471,"user":{"displayName":"Jeongmin Ahn","userId":"13913237267246789835"}},"outputId":"839edbd4-fb80-4bd5-c805-355ff4a20876"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["전체 샘플 중 길이가 500 이하인 샘플의 비율: 87.795\n"]}]},{"cell_type":"code","source":["def pad_sequences(sentences, max_len):\n","  features = np.zeros((len(sentences), max_len), dtype=int)\n","  for index, sentence in enumerate(sentences):\n","    if len(sentence) != 0:\n","      features[index, :len(sentence)] = np.array(sentence)[:max_len]\n","  return features\n","\n","padded_X_train = pad_sequences(encoded_X_train, max_len=max_len)\n","padded_X_valid = pad_sequences(encoded_X_valid, max_len=max_len)\n","padded_X_test = pad_sequences(encoded_X_test, max_len=max_len)\n","\n","print('훈련 데이터의 크기 :', padded_X_train.shape)\n","print('검증 데이터의 크기 :', padded_X_valid.shape)\n","print('테스트 데이터의 크기 :', padded_X_test.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fClRdq9ZrDRy","executionInfo":{"status":"ok","timestamp":1721521432612,"user_tz":-540,"elapsed":1962,"user":{"displayName":"Jeongmin Ahn","userId":"13913237267246789835"}},"outputId":"7e40d33d-e086-4a4d-f750-3ebc91604a7e"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["훈련 데이터의 크기 : (20000, 500)\n","검증 데이터의 크기 : (5000, 500)\n","테스트 데이터의 크기 : (25000, 500)\n"]}]},{"cell_type":"markdown","source":["# 3. 모델 구현\n","\n","딥 러닝 프레임워크 PyTorch를 이용하여 GRU 모델을 구현"],"metadata":{"id":"7r2BntjbrYlQ"}},{"cell_type":"code","source":["USE_CUDA = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n","print(\"cpu와 cuda 중 다음 기기로 학습함:\", device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9_U87i0orF4q","executionInfo":{"status":"ok","timestamp":1721521536536,"user_tz":-540,"elapsed":842,"user":{"displayName":"Jeongmin Ahn","userId":"13913237267246789835"}},"outputId":"128c49b9-8cc6-4b84-ffb1-788745a7a5f5"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["cpu와 cuda 중 다음 기기로 학습함: cuda\n"]}]},{"cell_type":"code","source":["train_label_tensor = torch.tensor(np.array(y_train))\n","valid_label_tensor = torch.tensor(np.array(y_valid))\n","test_label_tensor = torch.tensor(np.array(y_test))\n","print(train_label_tensor[:5])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"814fplGwrfgk","executionInfo":{"status":"ok","timestamp":1721521548164,"user_tz":-540,"elapsed":498,"user":{"displayName":"Jeongmin Ahn","userId":"13913237267246789835"}},"outputId":"45bc0bfc-f36a-4846-91a9-67527850fad3"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1, 1, 0, 0, 0])\n"]}]},{"cell_type":"markdown","source":["- 단어 벡터의 차원 = 100\n","- 문장 길이 = 500\n","- 배치 크기 = 32\n","- 데이터 개수 = 2만\n","- GRU의 은닉층의 크기 = 128\n","- 분류하고자 하는 카테고리 개수 = 2개 (긍정, 부정)"],"metadata":{"id":"hWSjdSPOrt-F"}},{"cell_type":"code","source":["class TextClassifier(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n","        super(TextClassifier, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim) # output_dim = 분류하고자하는 카테고리의 개수\n","\n","    def forward(self, x):\n","        # x: (batch_size, seq_length) == (32, 500)\n","        embedded = self.embedding(x)  # (batch_size, seq_length, embedding_dim) == (32, 500, 100) == (데이터의 개수, 문장길이, 단어 벡터의 차원)\n","        gru_out, hidden = self.gru(embedded)  # gru_out: (batch_size, seq_length, hidden_dim), hidden: (1, batch_size, hidden_dim)\n","        last_hidden = hidden.squeeze(0)  # (batch_size, hidden_dim)\n","        logits = self.fc(last_hidden)  # (batch_size, output_dim)\n","        return logits"],"metadata":{"id":"SMPChao9riba","executionInfo":{"status":"ok","timestamp":1721521726940,"user_tz":-540,"elapsed":888,"user":{"displayName":"Jeongmin Ahn","userId":"13913237267246789835"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["(인코더) (32, 500) => 입력 데이터의 형태 => 임베딩 층 통과 후 => (32, 500, 100) => GRU 통과 후 => (32, 128) => Softmax 출력층 통과 후 => (32, 2)"],"metadata":{"id":"u_XAB__asE5U"}},{"cell_type":"code","source":["# 파이토치 텐서로 변환하고 배치 단위 연산을 위해 데이터로더로 변환\n","encoded_train = torch.tensor(padded_X_train).to(torch.int64)\n","train_dataset = torch.utils.data.TensorDataset(encoded_train, train_label_tensor)\n","train_dataloader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=32)\n","\n","encoded_test = torch.tensor(padded_X_test).to(torch.int64)\n","test_dataset = torch.utils.data.TensorDataset(encoded_test, test_label_tensor)\n","test_dataloader = torch.utils.data.DataLoader(test_dataset, shuffle=True, batch_size=1)\n","\n","encoded_valid = torch.tensor(padded_X_valid).to(torch.int64)\n","valid_dataset = torch.utils.data.TensorDataset(encoded_valid, valid_label_tensor)\n","valid_dataloader = torch.utils.data.DataLoader(valid_dataset, shuffle=True, batch_size=1)"],"metadata":{"id":"hiEct-yesKhv","executionInfo":{"status":"ok","timestamp":1721521755909,"user_tz":-540,"elapsed":476,"user":{"displayName":"Jeongmin Ahn","userId":"13913237267246789835"}}},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":["=> 훈련 데이터의 샘플 개수가 20,000개 였으므로 배치 크기를 32로 할 경우에는 20000/32=625 다시 말해 32개씩 묶인 데이터 묶음이 625개 생성"],"metadata":{"id":"aBXmtQbJsc7m"}},{"cell_type":"code","source":["# 모델 객체 선언\n","embedding_dim = 100\n","hidden_dim = 128\n","output_dim = 2\n","learning_rate = 0.01\n","num_epochs = 10\n","\n","model = TextClassifier(vocab_size, embedding_dim, hidden_dim, output_dim)\n","model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SyUk62lHsVLX","executionInfo":{"status":"ok","timestamp":1721521813661,"user_tz":-540,"elapsed":1703,"user":{"displayName":"Jeongmin Ahn","userId":"13913237267246789835"}},"outputId":"420d6b07-fd9b-4d13-a323-bb8e0f7711ee"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TextClassifier(\n","  (embedding): Embedding(38711, 100)\n","  (gru): GRU(100, 128, batch_first=True)\n","  (fc): Linear(in_features=128, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["criterion = nn.CrossEntropyLoss() # 분류 문제\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"],"metadata":{"id":"XHCmsq13si-c","executionInfo":{"status":"ok","timestamp":1721521846106,"user_tz":-540,"elapsed":2946,"user":{"displayName":"Jeongmin Ahn","userId":"13913237267246789835"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["# 평가 코드 작성\n","def calculate_accuracy(logits, labels):\n","    # _, predicted = torch.max(logits, 1)\n","    predicted = torch.argmax(logits, dim=1)\n","    correct = (predicted == labels).sum().item()\n","    total = labels.size(0)\n","    accuracy = correct / total\n","    return accuracy\n","\n","def evaluate(model, valid_dataloader, criterion, device):\n","    val_loss = 0\n","    val_correct = 0\n","    val_total = 0\n","\n","    model.eval()\n","    with torch.no_grad():\n","        # 데이터로더로부터 배치 크기만큼의 데이터를 연속으로 로드\n","        for batch_X, batch_y in valid_dataloader:\n","            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n","\n","            # 모델의 예측값\n","            logits = model(batch_X)\n","\n","            # 손실을 계산\n","            loss = criterion(logits, batch_y)\n","\n","            # 정확도와 손실을 계산함\n","            val_loss += loss.item()\n","            val_correct += calculate_accuracy(logits, batch_y) * batch_y.size(0)\n","            val_total += batch_y.size(0)\n","\n","    val_accuracy = val_correct / val_total\n","    val_loss /= len(valid_dataloader)\n","\n","    return val_loss, val_accuracy\n"],"metadata":{"id":"eoUhsSn-sqlE","executionInfo":{"status":"ok","timestamp":1721521923723,"user_tz":-540,"elapsed":480,"user":{"displayName":"Jeongmin Ahn","userId":"13913237267246789835"}}},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":["## 4. 모델 학습"],"metadata":{"id":"y7OfKFv0tACe"}},{"cell_type":"code","source":["num_epochs = 5\n","\n","# Training loop\n","best_val_loss = float('inf')\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    # Training\n","    train_loss = 0\n","    train_correct = 0\n","    train_total = 0\n","    model.train()\n","    for batch_X, batch_y in train_dataloader:\n","        # Forward pass\n","        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n","        # batch_X.shape == (batch_size, max_len)\n","        logits = model(batch_X)\n","\n","        # Compute loss\n","        loss = criterion(logits, batch_y)\n","\n","        # Backward pass and optimization\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Calculate training accuracy and loss\n","        train_loss += loss.item()\n","        train_correct += calculate_accuracy(logits, batch_y) * batch_y.size(0)\n","        train_total += batch_y.size(0)\n","\n","    train_accuracy = train_correct / train_total\n","    train_loss /= len(train_dataloader)\n","\n","    # Validation\n","    val_loss, val_accuracy = evaluate(model, valid_dataloader, criterion, device)\n","\n","    print(f'Epoch {epoch+1}/{num_epochs}:')\n","    print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n","    print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n","\n","    # 검증 손실이 최소일 때 체크포인트 저장\n","    if val_loss < best_val_loss:\n","        print(f'Validation loss improved from {best_val_loss:.4f} to {val_loss:.4f}. 체크포인트를 저장합니다.')\n","        best_val_loss = val_loss\n","        torch.save(model.state_dict(), 'best_model_checkpoint.pth')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ukV0CLOjs-JK","executionInfo":{"status":"ok","timestamp":1721522004264,"user_tz":-540,"elapsed":64231,"user":{"displayName":"Jeongmin Ahn","userId":"13913237267246789835"}},"outputId":"fa6c228c-94cc-4dea-fbbe-a522b8013f20"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5:\n","Train Loss: 0.6974, Train Accuracy: 0.4979\n","Validation Loss: 0.6965, Validation Accuracy: 0.4984\n","Validation loss improved from inf to 0.6965. 체크포인트를 저장합니다.\n","Epoch 2/5:\n","Train Loss: 0.6912, Train Accuracy: 0.5139\n","Validation Loss: 0.6967, Validation Accuracy: 0.4994\n","Epoch 3/5:\n","Train Loss: 0.6747, Train Accuracy: 0.5447\n","Validation Loss: 0.6487, Validation Accuracy: 0.6402\n","Validation loss improved from 0.6965 to 0.6487. 체크포인트를 저장합니다.\n","Epoch 4/5:\n","Train Loss: 0.5400, Train Accuracy: 0.7345\n","Validation Loss: 0.4256, Validation Accuracy: 0.8164\n","Validation loss improved from 0.6487 to 0.4256. 체크포인트를 저장합니다.\n","Epoch 5/5:\n","Train Loss: 0.2964, Train Accuracy: 0.8812\n","Validation Loss: 0.3246, Validation Accuracy: 0.8714\n","Validation loss improved from 0.4256 to 0.3246. 체크포인트를 저장합니다.\n"]}]},{"cell_type":"markdown","source":["# 5. 모델 로드 및 평가"],"metadata":{"id":"ISY5IZWKtFiN"}},{"cell_type":"code","source":["# 모델 로드\n","model.load_state_dict(torch.load('best_model_checkpoint.pth'))\n","\n","# 모델을 device에 올립니다.\n","model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w4sYV7rMtCPw","executionInfo":{"status":"ok","timestamp":1721522017718,"user_tz":-540,"elapsed":514,"user":{"displayName":"Jeongmin Ahn","userId":"13913237267246789835"}},"outputId":"8a30e668-daed-4930-f713-588317dd404a"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TextClassifier(\n","  (embedding): Embedding(38711, 100)\n","  (gru): GRU(100, 128, batch_first=True)\n","  (fc): Linear(in_features=128, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["# 검증 데이터에 대한 정확도와 손실 계산\n","val_loss, val_accuracy = evaluate(model, valid_dataloader, criterion, device)\n","\n","print(f'Best model validation loss: {val_loss:.4f}')\n","print(f'Best model validation accuracy: {val_accuracy:.4f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_DS0BU45tIDx","executionInfo":{"status":"ok","timestamp":1721522026433,"user_tz":-540,"elapsed":7883,"user":{"displayName":"Jeongmin Ahn","userId":"13913237267246789835"}},"outputId":"765de49f-4630-4f4f-f136-3d5180d8ceda"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Best model validation loss: 0.3246\n","Best model validation accuracy: 0.8714\n"]}]},{"cell_type":"code","source":["# 테스트 데이터에 대한 정확도와 손실 계산\n","test_loss, test_accuracy = evaluate(model, test_dataloader, criterion, device)\n","\n","print(f'Best model test loss: {test_loss:.4f}')\n","print(f'Best model test accuracy: {test_accuracy:.4f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BSdnODF9tH_z","executionInfo":{"status":"ok","timestamp":1721522065702,"user_tz":-540,"elapsed":39285,"user":{"displayName":"Jeongmin Ahn","userId":"13913237267246789835"}},"outputId":"b327acb9-c423-476e-ef78-8ded9d5c65a7"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Best model test loss: 0.3349\n","Best model test accuracy: 0.8613\n"]}]},{"cell_type":"markdown","source":["## 6. 구조적 파악"],"metadata":{"id":"h0KmmmUXy1X2"}},{"cell_type":"markdown","source":["https://github.com/bentrevett/pytorch-seq2seq"],"metadata":{"id":"pI8XP0Mxzbfw"}},{"cell_type":"code","source":["class Encoder(nn.Module):\n","    def __init__(self, input_dim, embedding_dim, hidden_dim, dropout):\n","        super().__init__()\n","        self.hidden_dim = hidden_dim\n","        self.embedding = nn.Embedding(input_dim, embedding_dim)\n","        self.rnn = nn.GRU(embedding_dim, hidden_dim)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, src):\n","        # src = [src length, batch size]\n","        embedded = self.dropout(self.embedding(src))\n","        # embedded = [src length, batch size, embedding dim]\n","        outputs, hidden = self.rnn(embedded)  # no cell state in GRU!\n","        # outputs = [src length, batch size, hidden dim * n directions]\n","        # hidden = [n layers * n directions, batch size, hidden dim]\n","        # outputs are always from the top hidden layer\n","        return hidden"],"metadata":{"id":"cjsHr0TdtH9o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Decoder(nn.Module):\n","    def __init__(self, output_dim, embedding_dim, hidden_dim, dropout):\n","        super().__init__()\n","        self.hidden_dim = hidden_dim\n","        self.output_dim = output_dim\n","        self.embedding = nn.Embedding(output_dim, embedding_dim)\n","        self.rnn = nn.GRU(embedding_dim + hidden_dim, hidden_dim)\n","        self.fc_out = nn.Linear(embedding_dim + hidden_dim * 2, output_dim)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, input, hidden, context):\n","        # input = [batch size]\n","        # hidden = [n layers * n directions, batch size, hidden dim]\n","        # context = [n layers * n directions, batch size, hidden dim]\n","        # n layers and n directions in the decoder will both always be 1, therefore:\n","        # hidden = [1, batch size, hidden dim]\n","        # context = [1, batch size, hidden dim]\n","        input = input.unsqueeze(0)\n","        # input = [1, batch size]\n","        embedded = self.dropout(self.embedding(input))\n","        # embedded = [1, batch size, embedding dim]\n","        emb_con = torch.cat((embedded, context), dim=2)\n","        # emb_con = [1, batch size, embedding dim + hidden dim]\n","        output, hidden = self.rnn(emb_con, hidden)\n","        # output = [seq len, batch size, hidden dim * n directions]\n","        # hidden = [n layers * n directions, batch size, hidden dim]\n","        # seq len, n layers and n directions will always be 1 in this decoder, therefore:\n","        # output = [1, batch size, hidden dim]\n","        # hidden = [1, batch size, hidden dim]\n","        output = torch.cat(\n","            (embedded.squeeze(0), hidden.squeeze(0), context.squeeze(0)), dim=1\n","        )\n","        # output = [batch size, embedding dim + hidden dim * 2]\n","        prediction = self.fc_out(output)\n","        # prediction = [batch size, output dim]\n","        return prediction, hidden"],"metadata":{"id":"SALbMA7ky3Nb"},"execution_count":null,"outputs":[]}]}